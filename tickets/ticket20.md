## 20.1 Решающие деревья. Функции информационного выигрыша. Алгоритм построения дерева.

Смотри презентацию [ML_10_Trees.pdf](https://docs.yandex.ru/docs/view?url=ya-disk-public%3A%2F%2FTce3Hg4R521%2FAeGvN14%2FuhhBJbYmfaf3PaCuY7embqZnn%2BiIO%2BBq00rZ5aTL40zE%2Bb3nCKLCVTJ%2BSInaOUvvHQ%3D%3D%3A%2F%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8%2FML_10_Trees.pdf&name=ML_10_Trees.pdf) ([папка](https://disk.yandex.ru/d/wyPHyCSqv_4ilg/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8)) (слайды 1-16).

Смотри лекцию (2024-12-09): таймкод 13:55 [YouTube](https://youtu.be/bZFIfWzVvUs?list=PLxMpIvWUjaJsttwLkYi-uEydy6R9Hk2-v&t=822), [Google Drive](https://drive.google.com/drive/folders/1oAid_KeLC9P-_mvzrL-TT223MxnWHQB8).

---

**Решающие деревья** (или дерево решений) представляют собой алгоритм машинного обучения, основанный на правилах, что обеспечивает их **интерпретируемость**. Это означает, что мы можем понять как результаты построения дерева, так и процесс принятия решений. Одним из преимуществ является отсутствие необходимости в предобработке признаков, таких как нормировка или one-hot encoding, что позволяет работать как с числовыми, так и с категориальными данными. Решающие деревья также устойчивы к изменениям в данных.

### Функции информационного выигрыша

**Информационный выигрыш (IG)** используется для оценки полезности разбиения данных в узле дерева. Он определяется по формуле:

$$
IG = \frac{|X_{\text{node}}|}{|X_{\text{total}}|} I(X_{\text{node}}) - \frac{|X_{\text{right}}|}{|X_{\text{total}}|} I(X_{\text{right}}) - \frac{|X_{\text{left}}|}{|X_{\text{total}}|} I(X_{\text{left}})
$$

где $I$ — это функция нечистоты (impurity), которая показывает, насколько узел "чист" (т.е. содержит объекты одного класса). Чем ближе значение к одному классу, тем меньше нечистота.

1. **Misclassification Error**:

$$
I_E(X) = 1 - \max\{p(y)\} = 1 - \max_y \left( \frac{|\{x_i: y_i = y\}|}{|X|} \right)
$$

2. **Entropy**:

$$
I_H(X) = - \sum_{y \in Y} p(y) \log_2(p(y)) = - \sum_{y \in Y} \frac{|\{x_i : y_i = y\}|}{|X|} \times \log_2 \left( \frac{|\{x_i : y_i = y\}|}{|X|} \right)
$$

3. **Gini Impurity**:

$$
I_G(X) = \sum_{y \in Y} p(y)(1 - p(y)) = \sum_{y \in Y} \left( \frac{|\{x_i : y_i = y\}|}{|X|} \right) \left( \frac{|\{x_i : y_i \neq y\}|}{|X|} \right)
$$

![Comparison](images/tickets20_1.png)


Для задач классификации обычно используется **Gini**, а для регрессии применяется другая метрика:

$$
I_V(X) = \sum_{x_i \in X} \sum_{x_j \in X} \frac{1}{2} (y_i - y_j)^2
$$

### Алгоритм построения дерева

1. Если все объекты в узле принадлежат одному классу, помечаем лист как этот класс и останавливаемся.
2. Ищем правило с наибольшим IG. Если ни одно правило не дает прироста информации, помечаем узел как принадлежащий к наибольшему классу и останавливаемся.
3. Разделяем узлы на детей по правилу.
4. Повторяем шаг 1 для каждого нового узла.

В случае регрессии возвращаем среднее значение вместо класса.







## 20.2 Глобальный поиск. Байесовская оптимизация. Функции выбора следующего измерения.


Смотри презентацию [ML_12_Bayes_and_DFO.pdf](https://docs.yandex.ru/docs/view?url=ya-disk-public%3A%2F%2FTce3Hg4R521%2FAeGvN14%2FuhhBJbYmfaf3PaCuY7embqZnn%2BiIO%2BBq00rZ5aTL40zE%2Bb3nCKLCVTJ%2BSInaOUvvHQ%3D%3D%3A%2F%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8%2FML_12_Bayes_and_DFO.pdf&name=ML_12_Bayes_and_DFO.pdf&nosw=1) ([папка](https://disk.yandex.ru/d/wyPHyCSqv_4ilg/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8)).

Смотри последнюю лекцию (2024-12-16): таймкод 01:03:53 [Google Drive](https://drive.google.com/drive/folders/10gvaRHs_NQW4v5ULc5b6L83-pPs2zq6g).

---

**Байесовская оптимизация** применяется в ситуациях, когда проведение множества экспериментов невозможно, например, при бурении нефтяных скважин. Основная идея заключается в проведении эксперимента и пересчете распределения значений функции на основе предыдущих экспериментов для оценки вероятности каждого значения. Хорошо подходит нормальное распределение для значений функции в точке. Чем ближе точка к измерению, тем ближе её матожидание к значению измерения и меньше дисперсия.

![Байесовская оптимизация](./images/tickets20_2.png)

### Стратегии выбора точки

Функции выбора определяют новую точку для измерения:

1. **Ожидаемое улучшение (Expected Improvement)**:

$$
EI(x) = \mathbb{E} [\max(f(x) - f(x_{\text{best}}), 0)]
$$

3. **Верхняя граница доверительного интервала (Upper Confidence Bound)**:

$$
UCB(x) = \mu(x) + \beta\sigma(x)
$$

   где $\beta$ — параметр, определяемый пользователем.

3. **Вероятность улучшения (Probability of Improvement)**:

$$
PI(x) = P(f(x) > f(x_{\text{best}}))
$$


## Предсказание на доп вопросы
### 1
В регрессии количество листьев в дереве или $2^{\text{глубина}}$ определяет количество значений функции, так как каждый лист дает новое значение на каком-то промежутке.

Виды регуляризации деревьев (на самом деле другой билет):

1. Ограничение глубины дерева.
2. Минимальное количество точек в листе (разделяем новый лист, если в нем больше $k$ точек).
3. Минимальное количество точек в узле (начинаем разделять, если в ней больше $k$ точек).

Меньшее дерево снижает риск переобучения.

### 2
Как сделать первое измерение и какие начальные параметы (\mu(x) и \sigma(x)). Ответ: Я точный ответ не знаю, но сказал бы так. Сделаем первое в одном краю. Значение функции это начальное значение для матожидания. Чем ближе к краю тем меньше дисперсия вплоть до 0. Осталась проблема. Для некоторых функций выбора бесконечно много точек с лучшим значением, т.к. одиннаковое матожидание. По всем функциям выбора другой край одна из наилучших. Давайте для второго измерения возьмём её. Теперь у всех точек разные дисперсии и матожидания. Можем работать по алгоритму
