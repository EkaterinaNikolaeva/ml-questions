## 14.1) Нейронные сети. Перцептрон. Логистическая регрессия и градиентный спуск.

Лекция: https://www.youtube.com/watch?v=kzhP504D4v8&list=PLxMpIvWUjaJsttwLkYi-uEydy6R9Hk2-v&index=7

Первая нейронная сеть - перцептрон. По сути модель для бинарной классификации. Функция активации - 
$f(x) = 1 \,\,\,\text{если}\,\,\, x > treshhold, -1\,\,\text{иначе}$
<img src="images/ticket14/image-1.png" alt="" width="25%" height="25%">

<img src="images/ticket14/image-2.png" alt="" width="25%" height="25%">

Сдвигаем прямую на $y_i||X_i||^2$.

1. Фичи – входные данные
3. Считаем $\sum w_ix_i$
4. Функция трешхолда (sign) – функция активации

Можно добавить много слоев перцептрона
Проблема: такие сети ограничены, хотим что-то умнее. Решение: возьмем другую функцию активации, заменим выходную функцию перцептрона на сигмоиду:$\\$
<img src="images/ticket14/image-3.png" alt="" width="25%" height="25%">

Понятно, как получить результат. Вероятность попасть в конкретный класс считается так:

<img src="images/ticket14/image-4.png" alt="" width="25%" height="25%">

Получаем логистическую регрессию (на самом деле решаем задачу классификации, функция нам дает чиселко - а мы по этой чиселке решаем, принадлежит ли точка классу или нет).

Раньше наш лосс - количество неправильно классифицированных точек. Теперь в непрерывном случае удобно рассматривать функцию правдоподобия (логарифм совместной плотности [negative log likely loss]):

<img src="images/ticket14/image-5.png" alt="" width="25%" height="25%">

Как обучать сеть - градиентный спуск по лоссу логистической регрессии
Хотим на каждом слое минимизировать функцию лосса. На $i$-м слое считаем градиент по нашим весам $w_i$. Спускаемся в обратном направлении градиента с шагом $\eta$.
<img src="images/ticket14/image-6.png" alt="" width="25%" height="25%">

Есть стохастический градиентный спуск - делим данные на батчи, в пределах одной эпохи считаем градиенты по очередному батчу и спускаемся. Повторяем, пока батчи не закончились.
Веса модели обновляем с помощью back propagation (обратное распространение ошибки) - идем сконца, считаем градиенты и фиксим веса. 

## 14.2) ЕМ-алгоритм.

