## 17.1 Трансформеры. Attention head, Q, K, V. Multi-head attention

$$
\tilde{E} \in \mathbb{R}^{n \times d} \quad \text{– матрица эмбеддингов, } n \text{ – длина последовательности, } 
d \text{ – внутренняя размерность (размерность матрицы K)}
$$

Даны матрицы $W_Q, W_K \in \mathbb{R}^{d \times d},W_V \in \mathbb{R}^{d \times v}$

Есть интуиция, что представление каждого токена должно отличаться в зависимости от того, считаем мы его собственный вектор attention или выясняем его связь с другим токеном, для которого считаем attention
Для первой ситуации заведем матрицу $W_Q$ (Queries), которая поменяет наш эмбеддинг перед "запросом" на его attention. Аналогично матрица $W_K$ (Keys) поменяет эмбеддинг под вторую ситуацию. Матрица $W_V$ это какая-то информация, которую несет в себе эмбеддинг. В зависимости от того, на что нужно "обратить внимание", мы берем больше или меньше этой информации из вектора. Итоговый output для i-го слова получается как сумма векторов из ${V}$ c весами из A[i] :
$$
{Q} = \tilde{E} \times W_Q, \quad {K} = \tilde{E} \times W_K, \quad {V} = \tilde{E} \times W_V
$$

$$
A = softmax(\frac{{K} \times {Q}^T}{\sqrt{d}}) \in \mathbb{R}^{n \times n}
$$
A - матрица внимания от каждого вектора к каждому
$$
H = A * {V}
$$
H - итоговый результат блока Attention. Иногда его тоже называют вниманием. Эта матрица состоит из векторов z с картинки:

<img src="images/17_1.png" width="600">


## Multi-head attention

Идея: сделаем k троек $(W_Q, W_K, W_V)$, поделив их вторые размерности на k. То есть, если считать, что у $W_V$ такая же размерность (что необязательно так, но удобно), как и у $W_Q, W_K$, то у нас k троек, где каждая матрица $W \in \mathbb{R}^{d \times \frac{d}{k}}$
Каждая тройка в теории будет выполнять разную функцию (отвечать за синтаксис, за смысл, за лексику) и т.п. 
Считаем с каждой тройкой Attention и конкатенируем результаты, из которых все нужное достаем еще одной обучаемой матрицей $W_O \in \mathbb{R}^{d \times d}$ получая итоговую матрицу H (Z на картинке)

<img src="images/17_2.png" width="600">
---
## 17.2 Локальный поиск. Hill Climb и его вариации

<img src="images/17_3.png" width="600">

Перебираем соседние варианты, считаем в них функцию и жадно движемся в сторону максимума.

Возможные проблемы: можем сойтись к локальному максимуму, откуда уже не выбраться.

Вариации:
Cтохастический hill climb:
идем с какой-то вероятностью, пропорциональной выгодности, а не просто жадно. Это позволит нам не застрять в локальном максимуме навсегда.

Taboo search:
1) нельзя возвращаться
2) не останавливаемся в максимуме, а просто идем сколько-то шагов, запоминая аргмакс.

Particle swarm optimization:
есть несколько параллельных алгоритмов, которые могут сообщать друг другу свои текущие результаты. Тогда каждый идет по сумме векторов своего максимума и глобального.

<img src="images/17_4.png" width="600">

## Отжиг

Уравниваем или наоборот вносим неравенство в вероятности с помощью "температуры" T. Чем она больше, тем равнее вероятности и наоборот.

<img src="images/17_5.png" width="600">

В процессе поиска имеет смысл несколько раз пройтись от высокой температуры к низкой.